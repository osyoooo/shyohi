import glob
import os
import re
import xml.etree.ElementTree as ET

import faiss
import numpy as np
import pandas as pd
import streamlit as st
from sentence_transformers import SentenceTransformer

def _extract_article_text(article: ET.Element) -> str:
    """Collect text from an Article element."""
    caption = article.findtext('ArticleCaption') or ''
    title = article.findtext('ArticleTitle') or ''
    sentences = [s.text.strip() for s in article.findall('.//Sentence') if s.text]
    body = ' '.join(sentences)
    return ' '.join([caption, title, body]).strip()


@st.cache_data
def load_tax_law_xml(path_pattern: str) -> pd.DataFrame:
    records = []
    for filepath in glob.glob(path_pattern):
        tree = ET.parse(filepath)
        root = tree.getroot()
        date = root.findtext('.//EffectiveDate')
        if not date:
            m = re.search(r'_(\d{8})_', os.path.basename(filepath))
            date = m.group(1) if m else 'ä¸æ˜'
        for art in root.findall('.//Article'):
            num = art.attrib.get('Num') or art.findtext('ArticleTitle') or 'ä¸æ˜'
            title = art.findtext('ArticleTitle') or ''
            caption = art.findtext('ArticleCaption') or ''
            txt = _extract_article_text(art)
            records.append({
                'id': os.path.basename(filepath) + '#' + str(num),
                'effective_date': date,
                'article': str(num),
                'title': title,
                'caption': caption,
                'article_label': f"{num} {title or caption}",
                'text': txt
            })
    return pd.DataFrame(records)

@st.cache_resource
def init_vector_index(df: pd.DataFrame):
    # multilingual model works better for Japanese queries
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    texts = df['text'].tolist()
    embs = model.encode(texts, convert_to_tensor=True)
    dim = embs.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embs.cpu().numpy())
    return model, index, embs

# â”€â”€â”€ Streamlit UI â”€â”€â”€
st.set_page_config(page_title="æ¶ˆè²»ç¨æ³•ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢", layout="wide")
st.title('ğŸ’¡ æ¶ˆè²»ç¨æ³•ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¢ãƒ—ãƒª')

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
df = load_tax_law_xml('data/*.xml')
model, index, embs = init_vector_index(df)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šçµã‚Šè¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
st.sidebar.header('ğŸ” çµã‚Šè¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼')
dates = st.sidebar.multiselect('æ–½è¡Œæ—¥', sorted(df['effective_date'].unique()))
arts  = st.sidebar.multiselect('æ¡æ–‡', sorted(df['article_label'].unique()))


# ãƒ¡ã‚¤ãƒ³ï¼šæ¤œç´¢ã‚¯ã‚¨ãƒª
query = st.text_input('ğŸ” ã”è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„')
synonyms = {'ä¸èª²ç¨': 'éèª²ç¨'}
search_query = query
for k, v in synonyms.items():
    search_query = search_query.replace(k, v)
top_k = st.slider('è¡¨ç¤ºä»¶æ•°', 1, 10, 5)

if query:
    q_emb = model.encode(search_query, convert_to_tensor=True).cpu().numpy()
    dists, idxs = index.search(q_emb.reshape(1, -1), len(df))
    results = df.iloc[idxs[0]]
    if dates:
        results = results[results['effective_date'].isin(dates)]
    if arts:
        results = results[results['article_label'].isin(arts)]
    results = results.head(top_k)
    st.markdown("### æ¤œç´¢çµæœ")
    for _, row in results.iterrows():
        with st.expander(f"æ¡æ–‡ {row['article_label']} ï¼ˆæ–½è¡Œæ—¥: {row['effective_date']}ï¼‰"):
            st.write(row['text'])

# ãƒ•ãƒƒã‚¿ãƒ¼
st.sidebar.markdown("---")
st.sidebar.markdown("**ä½¿ã„æ–¹**\n1. dataãƒ•ã‚©ãƒ«ãƒ€ã«XMLã‚’é…ç½®\n2. `streamlit run app.py` ã§èµ·å‹•\n")
